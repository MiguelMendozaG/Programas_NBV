{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\secie\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_input_vol = \"E:/dataset_tesis/dataset_volumes.npy\"\n",
    "path_input_lbl = \"E:/dataset_tesis/dataset_lbl_pose.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_vol = np.load(path_input_vol)\n",
    "dataset_lbl = np.load(path_input_lbl)\n",
    "dataset_lbl = (dataset_lbl+1)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volumes data size: \n",
      " (15166, 32768)\n",
      "Labels data size: \n",
      " (15166, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Volumes data size: \\n\",dataset_vol.shape)\n",
    "print(\"Labels data size: \\n\",dataset_lbl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 77.1566   92.15666 131.8851 ]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_lbl[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "baraja = list(zip(dataset_vol, dataset_lbl))\n",
    "shuffle(baraja)\n",
    "dataset_vol, dataset_lbl = zip(*baraja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[107.84334 131.8851  122.8434 ]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_lbl[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images size: \n",
      " (12132, 32768)\n",
      "Test images size: \n",
      " (3034, 32768)\n",
      "Training labels size: \n",
      " (12132, 3)\n",
      "Test labels size: \n",
      " (3034, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24988914470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 62.3868   97.59168 113.3958 ]\n",
      " [ 89.3797   96.71816 138.4245 ]\n",
      " [101.53144 129.2354  127.2571 ]\n",
      " [100.       65.974   121.0292 ]\n",
      " [125.086   130.3882  106.87359]\n",
      " [107.84334 131.8851  122.8434 ]]\n"
     ]
    }
   ],
   "source": [
    "vol_training = np.array(dataset_vol[0:int(0.8*len(dataset_vol))])\n",
    "vol_test = np.array(dataset_vol[int(0.8*len(dataset_vol)):])\n",
    "\n",
    "lbl_training = np.array(dataset_lbl[0:int(0.8*len(dataset_lbl))])\n",
    "lbl_test = np.array(dataset_lbl[int(0.8*len(dataset_lbl)):])\n",
    "\n",
    "print(\"Training images size: \\n\",vol_training.shape)\n",
    "print(\"Test images size: \\n\",vol_test.shape)\n",
    "\n",
    "print(\"Training labels size: \\n\",lbl_training.shape)\n",
    "print(\"Test labels size: \\n\",lbl_test.shape)\n",
    "\n",
    "indice = 1000\n",
    "slide = 0\n",
    "vol_slide = np.reshape(vol_training[indice],(32,32,32))\n",
    "vol_slide = vol_slide[:][:][slide]\n",
    "\n",
    "plt.imshow(vol_slide)\n",
    "plt.show()\n",
    "\n",
    "print(lbl_training[0:6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_red(x, n_classes = 3 ):\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    #limpiar graficas anteriores\n",
    "    #reset_graph()\n",
    "    \n",
    "    #Imagenes \n",
    "    img = tf.reshape(x, shape=[-1, 32,32,32])\n",
    "    #tf.summary.image(\"Image\", img)\n",
    "    # Declarando las variables \n",
    "    weights = {'W_conv1':tf.Variable(tf.truncated_normal([7,7,3,4], mean = mu, stddev = sigma)),\n",
    "               'W_conv2':tf.Variable(tf.truncated_normal([5,5,4,5], mean = mu, stddev = sigma)),\n",
    "               'W_fc1':tf.Variable(tf.truncated_normal([32*32*32,1500], mean = mu, stddev = sigma)),\n",
    "               'W_fc2':tf.Variable(tf.truncated_normal([1500, 750], mean = mu, stddev = sigma)),\n",
    "               'W_fc3':tf.Variable(tf.truncated_normal([750, 100], mean = mu, stddev = sigma)),\n",
    "               'W_fc4':tf.Variable(tf.truncated_normal([100,50], mean= mu, stddev= sigma)),\n",
    "               'out':tf.Variable(tf.truncated_normal([50, n_classes], mean = mu, stddev = sigma))}\n",
    "\n",
    "    biases = {'b_conv1':tf.Variable(tf.random_normal([4])),\n",
    "              'b_conv2':tf.Variable(tf.random_normal([5])),\n",
    "              'b_fc1':tf.Variable(tf.random_normal([1500])),\n",
    "              'b_fc2':tf.Variable(tf.random_normal([750])),\n",
    "              'b_fc3':tf.Variable(tf.random_normal([100])),\n",
    "              'b_fc4':tf.Variable(tf.random_normal([50])),\n",
    "              'out':tf.Variable(tf.random_normal([n_classes]))}\n",
    "   \n",
    "    \n",
    "    \n",
    "    # Declarando la arquitectura\n",
    "    \n",
    "    #Input: 200x200x3     Output: 50x50x3\n",
    "    #l1 = tf.nn.conv2d(img, weights['W_conv1'], strides=[1,2,2,1], padding='SAME')\n",
    "    #l1 = tf.add(l1, biases['b_conv1'])\n",
    "    #l1 = tf.nn.relu(l1)\n",
    "    #l1 = tf.nn.dropout(l1, keep_rate2)\n",
    "    #l1 = tf.nn.max_pool(l1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    \n",
    "    #Input: 50x50x3     Output: 50x50x6\n",
    "    #l2 = tf.nn.conv2d(l1, weights['W_conv2'], strides=[1,1,1,1], padding='SAME')\n",
    "    #l2 = tf.add(l2, biases['b_conv2'])\n",
    "    #l2 = tf.nn.relu(l2)\n",
    "    #l2 = tf.nn.dropout(l2, keep_rate2)\n",
    "    \n",
    "    #Input: 50x50x6     Output: 25x25x6\n",
    "    #l2 = tf.nn.max_pool(l2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    #print(l2.shape)\n",
    "    \n",
    "    \n",
    "    #Input: 6x6x8     Output: 128\n",
    "    #fc1 = tf.reshape(l2, [-1, 25*25*5])\n",
    "    fc1 = tf.nn.relu(tf.matmul(x, weights['W_fc1'])+biases['b_fc1'])\n",
    "    fc1 = tf.nn.dropout(fc1, keep_rate)\n",
    "                          \n",
    "    #Input: 128     Output: 64\n",
    "    fc2 = tf.nn.relu(tf.matmul(fc1, weights['W_fc2'])+biases['b_fc2'])\n",
    "    fc2 = tf.nn.dropout(fc2, keep_rate)\n",
    "    \n",
    "    fc3 = tf.nn.relu(tf.matmul(fc2, weights['W_fc3'])+biases['b_fc3'])\n",
    "    fc3 = tf.nn.dropout(fc3, keep_rate)\n",
    "    \n",
    "    fc4 = tf.nn.relu(tf.matmul(fc3, weights['W_fc4'])+biases['b_fc4'])\n",
    "    fc4 = tf.nn.dropout(fc4, keep_rate)\n",
    "\n",
    "    output_ = tf.matmul(fc4, weights['out'])+biases['out']\n",
    "\n",
    "    # Declarando la funcion de costo y entrenamiento\n",
    "    #cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y) )\n",
    "    #optimizer = tf.train.AdamOptimizer(0.0001).minimize(cost)\n",
    "    \n",
    "    #almacenar costo\n",
    "    #tf.summary.scalar(\"costo\", cost)\n",
    "    #generar logs\n",
    "    #summaries = tf.summary.merge_all()\n",
    "    \n",
    "    return output_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voxnet(x, n_classes = 3 ):\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    #limpiar graficas anteriores\n",
    "    #reset_graph()\n",
    "    \n",
    "    #Imagenes \n",
    "    img = tf.reshape(x, shape=[-1, 32,32,32,1])\n",
    "    #tf.summary.image(\"Image\", img)\n",
    "    # Declarando las variables \n",
    "    \n",
    "    weights = {'W_conv1':tf.Variable(tf.truncated_normal([5,5,5,1,32], mean = mu, stddev = sigma)),\n",
    "               'W_conv2':tf.Variable(tf.truncated_normal([3,3,3,32,32], mean = mu, stddev = sigma)),\n",
    "               'W_conv3':tf.Variable(tf.truncated_normal([3,3,3,12,8], mean = mu, stddev = sigma)),\n",
    "               'W_fc1':tf.Variable(tf.truncated_normal([8*8*8*32,128], mean = mu, stddev = sigma)),\n",
    "               'W_fc2':tf.Variable(tf.truncated_normal([1500, 500], mean = mu, stddev = sigma)),\n",
    "               'W_fc3':tf.Variable(tf.truncated_normal([500, 100], mean = mu, stddev = sigma)),\n",
    "               'W_fc4':tf.Variable(tf.truncated_normal([100,50], mean= mu, stddev= sigma)),\n",
    "               'out':tf.Variable(tf.truncated_normal([128, n_classes], mean = mu, stddev = sigma))}\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = {'W_conv1':tf.Variable(tf.random_normal([5,5,5,1,32])),\n",
    "               'W_conv2':tf.Variable(tf.random_normal([3,3,3,32,32])),\n",
    "               'W_conv3':tf.Variable(tf.random_normal([3,3,3,12,8])),\n",
    "               'W_fc1':tf.Variable(tf.random_normal([8*8*8*32,128])),\n",
    "               'W_fc2':tf.Variable(tf.random_normal([1500, 500])),\n",
    "               'W_fc3':tf.Variable(tf.random_normal([500, 100])),\n",
    "               'W_fc4':tf.Variable(tf.random_normal([100,50])),\n",
    "               'out':tf.Variable(tf.random_normal([128, n_classes]))}\n",
    "    \"\"\"\n",
    "\n",
    "    biases = {'b_conv1':tf.Variable(tf.random_normal([32])),\n",
    "              'b_conv2':tf.Variable(tf.random_normal([32])),\n",
    "              'b_conv3':tf.Variable(tf.random_normal([8])),\n",
    "              'b_fc1':tf.Variable(tf.random_normal([128])),\n",
    "              'b_fc2':tf.Variable(tf.random_normal([500])),\n",
    "              'b_fc3':tf.Variable(tf.random_normal([100])),\n",
    "              'b_fc4':tf.Variable(tf.random_normal([50])),\n",
    "              'out':tf.Variable(tf.random_normal([n_classes]))}\n",
    "   \n",
    "    \n",
    "    \n",
    "    # Declarando la arquitectura\n",
    "    \n",
    "    #Input: 200x200x3     Output: 50x50x3\n",
    "    l1 = tf.nn.conv3d(img, weights['W_conv1'], strides=[1,2,2,2,1], padding='SAME')\n",
    "    l1 = tf.add(l1, biases['b_conv1'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "    #l1 = tf.nn.leaky_relu(l1)\n",
    "    print(\"l1: \",l1.shape)\n",
    "    #l1 = tf.nn.dropout(l1, keep_rate)\n",
    "    #l1 = tf.nn.max_pool3d(l1, ksize=[1,2,2,2,1], strides=[1,2,2,2,1], padding='VALID')\n",
    "    \n",
    "    #Input: 50x50x3     Output: 50x50x6\n",
    "    l2 = tf.nn.conv3d(l1, weights['W_conv2'], strides=[1,1,1,1,1], padding='SAME')\n",
    "    l2 = tf.add(l2, biases['b_conv2'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "    #l2 = tf.nn.leaky_relu(l2)\n",
    "    #l2 = tf.nn.dropout(l2, keep_rate)\n",
    "    #print(l2)\n",
    "    \n",
    "    #Input: 50x50x6     Output: 25x25x6\n",
    "    l2 = tf.nn.max_pool3d(l2, ksize=[1,2,2,2,1], strides=[1,2,2,2,1], padding='VALID')\n",
    "    print(\"l2: \",l2.shape)\n",
    "    \n",
    "    #l3 = tf.nn.conv3d(l2, weights['W_conv3'], strides=[1,1,1,1,1], padding='SAME')\n",
    "    #l3= tf.add(l3, biases['b_conv3'])\n",
    "    #l3 = tf.nn.relu(l3)\n",
    "    #l2 = tf.nn.dropout(l2, keep_rate2)\n",
    "    #print(l2)\n",
    "    \n",
    "    #Input: 50x50x6     Output: 25x25x6\n",
    "    #l3 = tf.nn.max_pool3d(l3, ksize=[1,2,2,2,1], strides=[1,2,2,2,1], padding='VALID')\n",
    "    #print(l3.shape)\n",
    "    \n",
    "    \n",
    "    #Input: 6x6x8     Output: 128\n",
    "    fc1 = tf.reshape(l2, [-1, 8*8*8*32])\n",
    "    fc1 = tf.nn.relu(tf.matmul(fc1, weights['W_fc1'])+biases['b_fc1'])\n",
    "    #fc1 = tf.nn.leaky_relu(tf.matmul(fc1, weights['W_fc1'])+biases['b_fc1'])\n",
    "    fc1 = tf.nn.dropout(fc1, keep_rate)\n",
    "                          \n",
    "    #Input: 128     Output: 64\n",
    "    #fc2 = tf.nn.relu(tf.matmul(fc1, weights['W_fc2'])+biases['b_fc2'])\n",
    "    #fc2 = tf.nn.dropout(fc2, keep_rate)\n",
    "    \n",
    "    #fc3 = tf.nn.relu(tf.matmul(fc2, weights['W_fc3'])+biases['b_fc3'])\n",
    "    #fc3 = tf.nn.dropout(fc3, keep_rate)\n",
    "    \n",
    "    #fc4 = tf.nn.relu(tf.matmul(fc3, weights['W_fc4'])+biases['b_fc4'])\n",
    "    #fc4 = tf.nn.dropout(fc4, keep_rate)\n",
    "\n",
    "    output_ = tf.matmul(fc1, weights['out'])+biases['out']\n",
    "\n",
    "    # Declarando la funcion de costo y entrenamiento\n",
    "    #cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y) )\n",
    "    #optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    #almacenar costo\n",
    "    #tf.summary.scalar(\"costo\", cost)\n",
    "    #generar logs\n",
    "    #summaries = tf.summary.merge_all()\n",
    "    \n",
    "    return output_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def red_3d(x, n_classes = 3 ):\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    #limpiar graficas anteriores\n",
    "    #reset_graph()  \n",
    "    #Imagenes \n",
    "    img = tf.reshape(x, shape=[-1, 32,32,32,1])\n",
    "    #tf.summary.image(\"Image\", img)\n",
    "    # Declarando las variables \n",
    "    weights = {'W_conv1':tf.Variable(tf.truncated_normal([3,3,3,1,10], mean = mu, stddev = sigma)),\n",
    "               'W_conv2':tf.Variable(tf.truncated_normal([3,3,3,10,12], mean = mu, stddev = sigma)),\n",
    "               'W_conv3':tf.Variable(tf.truncated_normal([3,3,3,12,8], mean = mu, stddev = sigma)),\n",
    "               'W_fc1':tf.Variable(tf.truncated_normal([4*4*4*8,1500], mean = mu, stddev = sigma)),\n",
    "               'W_fc2':tf.Variable(tf.truncated_normal([1500, 500], mean = mu, stddev = sigma)),\n",
    "               'W_fc3':tf.Variable(tf.truncated_normal([500, 100], mean = mu, stddev = sigma)),\n",
    "               'W_fc4':tf.Variable(tf.truncated_normal([100,50], mean= mu, stddev= sigma)),\n",
    "               'out':tf.Variable(tf.truncated_normal([50, n_classes], mean = mu, stddev = sigma))}\n",
    "\n",
    "    biases = {'b_conv1':tf.Variable(tf.random_normal([10])),\n",
    "              'b_conv2':tf.Variable(tf.random_normal([12])),\n",
    "              'b_conv3':tf.Variable(tf.random_normal([8])),\n",
    "              'b_fc1':tf.Variable(tf.random_normal([1500])),\n",
    "              'b_fc2':tf.Variable(tf.random_normal([500])),\n",
    "              'b_fc3':tf.Variable(tf.random_normal([100])),\n",
    "              'b_fc4':tf.Variable(tf.random_normal([50])),\n",
    "              'out':tf.Variable(tf.random_normal([n_classes]))}\n",
    "   \n",
    "    \n",
    "    \n",
    "    # Declarando la arquitectura\n",
    "    \n",
    "    #Input: 200x200x3     Output: 50x50x3\n",
    "    l1 = tf.nn.conv3d(img, weights['W_conv1'], strides=[1,1,1,1,1], padding='SAME')\n",
    "    l1 = tf.add(l1, biases['b_conv1'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "    print(l1.shape)\n",
    "    #l1 = tf.nn.dropout(l1, keep_rate2)\n",
    "    l1 = tf.nn.max_pool3d(l1, ksize=[1,2,2,2,1], strides=[1,2,2,2,1], padding='VALID')\n",
    "    \n",
    "    #Input: 50x50x3     Output: 50x50x6\n",
    "    l2 = tf.nn.conv3d(l1, weights['W_conv2'], strides=[1,1,1,1,1], padding='SAME')\n",
    "    l2 = tf.add(l2, biases['b_conv2'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "    #l2 = tf.nn.dropout(l2, keep_rate2)\n",
    "    #print(l2)\n",
    "    \n",
    "    #Input: 50x50x6     Output: 25x25x6\n",
    "    l2 = tf.nn.max_pool3d(l2, ksize=[1,2,2,2,1], strides=[1,2,2,2,1], padding='VALID')\n",
    "    print(l2.shape)\n",
    "    \n",
    "    l3 = tf.nn.conv3d(l2, weights['W_conv3'], strides=[1,1,1,1,1], padding='SAME')\n",
    "    l3= tf.add(l3, biases['b_conv3'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "    #l3 = tf.nn.dropout(l3, keep_rate)\n",
    "    #print(l2)\n",
    "    \n",
    "    #Input: 50x50x6     Output: 25x25x6\n",
    "    l3 = tf.nn.max_pool3d(l3, ksize=[1,2,2,2,1], strides=[1,2,2,2,1], padding='VALID')\n",
    "    print(l3.shape)\n",
    "    \n",
    "    \n",
    "    #Input: 6x6x8     Output: 128\n",
    "    fc1 = tf.reshape(l3, [-1, 4*4*4*8])\n",
    "    fc1 = tf.nn.relu(tf.matmul(fc1, weights['W_fc1'])+biases['b_fc1'])\n",
    "    #fc1 = tf.nn.dropout(fc1, keep_rate)\n",
    "                          \n",
    "    #Input: 128     Output: 64\n",
    "    fc2 = tf.nn.relu(tf.matmul(fc1, weights['W_fc2'])+biases['b_fc2'])\n",
    "    #fc2 = tf.nn.dropout(fc2, keep_rate)\n",
    "    \n",
    "    fc3 = tf.nn.relu(tf.matmul(fc2, weights['W_fc3'])+biases['b_fc3'])\n",
    "    #fc3 = tf.nn.dropout(fc3, keep_rate)\n",
    "    \n",
    "    fc4 = tf.nn.relu(tf.matmul(fc3, weights['W_fc4'])+biases['b_fc4'])\n",
    "    fc4 = tf.nn.dropout(fc4, keep_rate)\n",
    "\n",
    "    output_ = tf.matmul(fc4, weights['out'])+biases['out']\n",
    "    \n",
    "    #almacenar costo\n",
    "    #tf.summary.scalar(\"costo\", cost)\n",
    "    #generar logs\n",
    "    #summaries = tf.summary.merge_all()\n",
    "    \n",
    "    return output_\n",
    "        #saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1:  (?, 16, 16, 16, 32)\n",
      "l2:  (?, 8, 8, 8, 32)\n"
     ]
    }
   ],
   "source": [
    "# Declarando las entradas y salidas\n",
    "x=tf.placeholder('float',[None,32*32*32])\n",
    "y=tf.placeholder('float')\n",
    "keep_rate = tf.placeholder(tf.float32)\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "output = voxnet(x)#################################\n",
    "\n",
    "cost = tf.reduce_mean( tf.squared_difference(output, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "#correct = tf.equal(tf.argmax(output, 1), tf.argmax(y, 1))\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct, 'float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(x_data, y_data, eval_batch_size):\n",
    "    sess = tf.get_default_session()\n",
    "    eval_i = 0\n",
    "    total_cost = 0\n",
    "    count = 0\n",
    "    for _ in range(int(len(x_data)/eval_batch_size)):\n",
    "        eval_x = x_data[eval_i:eval_i + eval_batch_size]\n",
    "        eval_y = y_data[eval_i:eval_i + eval_batch_size]\n",
    "        eval_cost = sess.run(cost, feed_dict = {x:eval_x, y:eval_y, keep_rate: 1, learning_rate: 0.0005})\n",
    "        total_cost += eval_cost\n",
    "        eval_i = eval_i + eval_batch_size\n",
    "        count = count  + 1\n",
    "    total_cost = total_cost / count \n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicted_output(x_data, y_data, k_rate = 1):\n",
    "    sess = tf.get_default_session()\n",
    "    y_predicted = []\n",
    "    for i in range(len(x_data)):\n",
    "        val = sess.run(output, feed_dict={x: x_data[i:i+1], y:y_data[i:i+1], keep_rate: k_rate, learning_rate: 0.0005})\n",
    "        y_predicted.append(val)\n",
    "    return y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(epochs = 600, batch_size = 100, save = False):\n",
    "    saver = tf.train.Saver()\n",
    "    start_time = time.time()\n",
    "    train_accuracy = []\n",
    "    test_error = []\n",
    "    epoch_loss_ = []\n",
    "    l_r = 0.001\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        #writer = tf.summary.FileWriter(\"./Octree_logs\")\n",
    "        #tf.summary.FileWriter.add_graph(writer,sess.graph)\n",
    "        #writer.add_graph(sess.graph)\n",
    "        i_prin = 0\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            i = 0\n",
    "            count = 0\n",
    "            if (epoch%100 == 0 and epoch > 0):\n",
    "                l_r = l_r - 0.0001\n",
    "            for _ in range(int(len(vol_training)/batch_size)):\n",
    "                epoch_x = vol_training[i:i+batch_size]\n",
    "                epoch_y = lbl_training[i:i+batch_size]\n",
    "                sess.run(optimizer ,feed_dict={x: epoch_x, y: epoch_y, keep_rate: 0.55, learning_rate: l_r})\n",
    "                c = sess.run(cost, feed_dict = {x: epoch_x, y: epoch_y, keep_rate: 1, learning_rate: 0.0005})\n",
    "                epoch_loss += c\n",
    "                i =i+batch_size\n",
    "                count += 1\n",
    "            \n",
    "            epoch_loss_.append(epoch_loss/count)\n",
    "            test_error.append(evaluate(vol_test, lbl_test, batch_size))\n",
    "            \n",
    "            if (epoch%20 == 0):\n",
    "                pass\n",
    "                #writer.add_summary(epoch_loss, epoch).eval()\n",
    "            if (epoch%50 ==0):\n",
    "                if isinstance(save, bool):\n",
    "                    ENCname=\"./SVEoctree_reg/\"+str(epoch)+\".ckpt\"\n",
    "                    saver.save(sess, ENCname)\n",
    "\n",
    "            print(\"Epoch: \", epoch,\"lr: \",l_r ,\"\\n \\t Training Error: \", epoch_loss_[i_prin])\n",
    "            print(\"\\t Test Error: \", test_error[i_prin])\n",
    "            i_prin = i_prin + 1\n",
    "        #####  X,Y,Z predicted and true values #####\n",
    "        #final_train_predicted = predicted_output(vol_training, lbl_training, 1)\n",
    "        #final_test_predicted = predicted_output(vol_test, lbl_test, 1)\n",
    "\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    return epoch_loss_, test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 lr:  0.001 \n",
      " \t Training Error:  674.2150680541993\n",
      "\t Test Error:  499.23218790690106\n",
      "Epoch:  1 lr:  0.001 \n",
      " \t Training Error:  458.1810038248698\n",
      "\t Test Error:  453.2708374023438\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-c9e70043a9e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_nn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-c1267ee4ce45>\u001b[0m in \u001b[0;36mtrain_nn\u001b[1;34m(epochs, batch_size, save)\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mepoch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlbl_training\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                 \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mepoch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mepoch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_rate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.55\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0ml_r\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                 \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mepoch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mepoch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_rate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.0005\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m                 \u001b[0mepoch_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss, test_loss = train_nn(500, 200, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Plot error ########\n",
    "fig = plt.figure()\n",
    "fig, ax = plt.subplots()\n",
    "x_axe = np.arange(0,len(train_loss),1)\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Error')\n",
    "ax.plot(x_axe, train_loss, 'b', label = 'Train')\n",
    "ax.plot(x_axe, test_loss, 'r', label = 'Test')\n",
    "ax.set_ylim(top = 500, bottom= 0)\n",
    "ax.legend()\n",
    "#fig.show()\n",
    "#fig.savefig(\"voxnet_truncated_normal_20_esc100_leak_relu.jpg\", dpi = 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_neural_network():\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        model_path = 'SVEoctree_reg/450.ckpt'\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver.restore(sess, model_path)\n",
    "        final_train_predicted = predicted_output(vol_training, lbl_training, 1)\n",
    "        final_test_predicted = predicted_output(vol_test, lbl_test, 1)\n",
    "        return final_train_predicted, final_test_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred, test_pred = test_neural_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Variables Definition ###\n",
    "\n",
    "####### Predicted ############\n",
    "train_pred = np.array(train_pred)\n",
    "test_pred = np.array(test_pred)\n",
    "\n",
    "x_train_pred = train_pred[:,0][:,0]\n",
    "y_train_pred = train_pred[:,0][:,1]\n",
    "z_train_pred = train_pred[:,0][:,2]\n",
    "\n",
    "x_test_pred = test_pred[:,0][:,0]\n",
    "y_test_pred = test_pred[:,0][:,1]\n",
    "z_test_pred = test_pred[:,0][:,2]\n",
    "\n",
    "####### True ################\n",
    "x_train_true = lbl_training[:,0]\n",
    "y_train_true = lbl_training[:,1]\n",
    "z_train_true = lbl_training[:,2]\n",
    "\n",
    "x_test_true = lbl_test[:,0]\n",
    "y_test_true = lbl_test[:,1]\n",
    "z_test_true = lbl_test[:,2]\n",
    "print(x_test_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "inf = 100\n",
    "sup = 300\n",
    "##### Plot x train######\n",
    "fig = plt.figure()\n",
    "fig, ax = plt.subplots()\n",
    "x_axe = np.arange(0,len(x_test_pred),1)\n",
    "#ax = fig.add_axes([1,1,1,1])\n",
    "ax.set_xlabel('Volume')\n",
    "ax.set_ylabel('Distance')\n",
    "ax.plot(x_axe, x_test_pred, label = 'x_predicted')\n",
    "ax.plot(x_axe, x_test_true, label = 'x_true')\n",
    "ax.set_xlim(left=inf, right=sup)\n",
    "#fig.show()\n",
    "#fig.savefig(\"x_test_function_esc100_relu.png\", dpi = 300)\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#error_function = ((x_test_pred - x_test_true)/100)-1\n",
    "error_function = ((x_test_pred/100)-1) - ((x_test_true/100)-1)\n",
    "x_axe_error = np.arange(0,len(error_function),1)\n",
    "fig = plt.figure()\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_axe_error, error_function)\n",
    "ax.set_xlim(left = inf, right=sup)\n",
    "#fig.show()\n",
    "#fig.savefig(\"error_esc_100_relu.png\", dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
